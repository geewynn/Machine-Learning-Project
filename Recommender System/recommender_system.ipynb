{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/godwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/godwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/godwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/godwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/godwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/godwin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "/home/godwin/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "rating = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "user = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "book = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating = pd.merge(rating, book, on='ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Year-Of-Publication', 'Publisher', 'Book-Author', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
    "book_rating.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count = (book_rating.\n",
    "     groupby(by = ['Book-Title'])['Book-Rating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'Book-Rating': 'RatingCount_book'})\n",
    "     [['Book-Title', 'RatingCount_book']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique books:  5850\n",
      "Number of unique users:  3192\n"
     ]
    }
   ],
   "source": [
    "threshold = 25\n",
    "rating_count = rating_count.query('RatingCount_book >= @threshold')\n",
    "\n",
    "user_rating = pd.merge(rating_count, book_rating, left_on='Book-Title', right_on='Book-Title', how='left')\n",
    "\n",
    "user_count = (user_rating.\n",
    "     groupby(by = ['User-ID'])['Book-Rating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'Book-Rating': 'RatingCount_user'})\n",
    "     [['User-ID', 'RatingCount_user']]\n",
    "    )\n",
    "    \n",
    "threshold = 20\n",
    "user_count = user_count.query('RatingCount_user >= @threshold')\n",
    "\n",
    "combined = user_rating.merge(user_count, left_on = 'User-ID', right_on = 'User-ID', how = 'inner')\n",
    "\n",
    "print('Number of unique books: ', combined['Book-Title'].nunique())\n",
    "print('Number of unique users: ', combined['User-ID'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "combined['Book-Rating'] = combined['Book-Rating'].values.astype(float)\n",
    "rating_scaled = pd.DataFrame(scaler.fit_transform(combined['Book-Rating'].values.reshape(-1,1)))\n",
    "combined['Book-Rating'] = rating_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/godwin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "combined = combined.drop_duplicates(['User-ID', 'Book-Title'])\n",
    "user_book_matrix = combined.pivot(index='User-ID', columns='Book-Title', values='Book-Rating')\n",
    "user_book_matrix.fillna(0, inplace=True)\n",
    "users = user_book_matrix.index.tolist()\n",
    "books = user_book_matrix.columns.tolist()\n",
    "user_book_matrix = user_book_matrix.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = combined['Book-Title'].nunique()\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "y_pred = decoder_op\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "pred_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 Loss: 0.341291239300927\n",
      "epoch: 2 Loss: 0.29368126703487646\n",
      "epoch: 3 Loss: 0.07927225994273693\n",
      "epoch: 4 Loss: 0.0038720811845141126\n",
      "epoch: 5 Loss: 0.0032519071547130306\n",
      "epoch: 6 Loss: 0.003072265689107743\n",
      "epoch: 7 Loss: 0.002948944124260119\n",
      "epoch: 8 Loss: 0.0029241058701690247\n",
      "epoch: 9 Loss: 0.002908087982847304\n",
      "epoch: 10 Loss: 0.0028969671776784317\n",
      "epoch: 11 Loss: 0.0028888531218559206\n",
      "epoch: 12 Loss: 0.002882708527997218\n",
      "epoch: 13 Loss: 0.0028779096376908187\n",
      "epoch: 14 Loss: 0.0028740734905823248\n",
      "epoch: 15 Loss: 0.002870946227071377\n",
      "epoch: 16 Loss: 0.0028683537141430183\n",
      "epoch: 17 Loss: 0.0028661744129219718\n",
      "epoch: 18 Loss: 0.0028643224715859026\n",
      "epoch: 19 Loss: 0.002862731308246461\n",
      "epoch: 20 Loss: 0.0028613538181941423\n",
      "epoch: 21 Loss: 0.002860148349050228\n",
      "epoch: 22 Loss: 0.0028590870545255943\n",
      "epoch: 23 Loss: 0.002858144957776908\n",
      "epoch: 24 Loss: 0.002857302401245541\n",
      "epoch: 25 Loss: 0.0028565467046164387\n",
      "epoch: 26 Loss: 0.0028558663504484752\n",
      "epoch: 27 Loss: 0.0028552481415937397\n",
      "epoch: 28 Loss: 0.0028546860474827035\n",
      "epoch: 29 Loss: 0.0028541721723421096\n",
      "epoch: 30 Loss: 0.002853701534619616\n",
      "epoch: 31 Loss: 0.0028532659878012733\n",
      "epoch: 32 Loss: 0.0028528688593184226\n",
      "epoch: 33 Loss: 0.0028525009101440946\n",
      "epoch: 34 Loss: 0.002852156970670426\n",
      "epoch: 35 Loss: 0.0028518377176414314\n",
      "epoch: 36 Loss: 0.0028515403686029898\n",
      "epoch: 37 Loss: 0.002851259574846743\n",
      "epoch: 38 Loss: 0.002850999732010558\n",
      "epoch: 39 Loss: 0.0028507574026442163\n",
      "epoch: 40 Loss: 0.00285052867468119\n",
      "epoch: 41 Loss: 0.002850313851313031\n",
      "epoch: 42 Loss: 0.0028501113756447213\n",
      "epoch: 43 Loss: 0.00284992112870236\n",
      "epoch: 44 Loss: 0.002849741520329409\n",
      "epoch: 45 Loss: 0.002849571371021179\n",
      "epoch: 46 Loss: 0.0028494091430719902\n",
      "epoch: 47 Loss: 0.0028492549938344204\n",
      "epoch: 48 Loss: 0.0028491089757593287\n",
      "epoch: 49 Loss: 0.002848968795081089\n",
      "epoch: 50 Loss: 0.0028488381745314204\n",
      "epoch: 51 Loss: 0.0028487133760271327\n",
      "epoch: 52 Loss: 0.002848593635832543\n",
      "epoch: 53 Loss: 0.0028484788055500978\n",
      "epoch: 54 Loss: 0.002848369270245862\n",
      "epoch: 55 Loss: 0.002848264719052547\n",
      "epoch: 56 Loss: 0.002848164553262785\n",
      "epoch: 57 Loss: 0.0028480662513974604\n",
      "epoch: 58 Loss: 0.002847973085878478\n",
      "epoch: 59 Loss: 0.0028478834703871674\n",
      "epoch: 60 Loss: 0.0028477980010723184\n",
      "epoch: 61 Loss: 0.002847716956818988\n",
      "epoch: 62 Loss: 0.0028476377176427908\n",
      "epoch: 63 Loss: 0.002847560541960155\n",
      "epoch: 64 Loss: 0.002847486323994267\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 35\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
    "    user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "        for batch in user_book_matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: user_book_matrix})\n",
    "\n",
    "    pred_data = pred_data.append(pd.DataFrame(preds))\n",
    "\n",
    "    pred_data = pred_data.stack().reset_index(name='Book-Rating')\n",
    "    pred_data.columns = ['User-ID', 'Book-Title', 'Book-Rating']\n",
    "    pred_data['User-ID'] = pred_data['User-ID'].map(lambda value: users[value])\n",
    "    pred_data['Book-Title'] = pred_data['Book-Title'].map(lambda value: books[value])\n",
    "    \n",
    "    keys = ['User-ID', 'Book-Title']\n",
    "    index_1 = pred_data.set_index(keys).index\n",
    "    index_2 = combined.set_index(keys).index\n",
    "\n",
    "    top_ten_ranked = pred_data[~index_1.isin(index_2)]\n",
    "    top_ten_ranked = top_ten_ranked.sort_values(['User-ID', 'Book-Rating'], ascending=[True, False])\n",
    "    top_ten_ranked = top_ten_ranked.groupby('User-ID').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_ranked.loc[top_ten_ranked['User-ID'] == 278582]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating.loc[book_rating['User-ID'] == 278582].sort_values(by=['Book-Rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
