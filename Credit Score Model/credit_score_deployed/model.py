# -*- coding: utf-8 -*-
"""model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PV7Y0KO2hzuhHsmE6-QoirOp_hGMHzso
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix, precision_recall_curve, roc_curve, auc

import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import LabelEncoder

from google.colab import files
!pip install kaggle

!mkdir .kaggle
!touch .kaggle/kaggle.json
!mv .kaggle /root/


#upload kaggle json file

uploaded = files.upload()

api_token = {"username":"geewynn","key":"b5dd5e9253a91c876fe288606e440250"}

import json
import zipfile

import os
with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 /content/.kaggle/kaggle.json
!kaggle config path -p /content

!kaggle datasets download -d wordsforthewise/lending-club

!unzip '/content/lending-club.zip'

accepted = pd.read_csv('/content/accepted_2007_to_2018q4.csv/accepted_2007_to_2018Q4.csv')

# check the missing value
missing = accepted.isna().mean() *100

#select columns with less than 30% of its data missing
new_df =accepted.loc[:, accepted.isnull().mean() < .30]


#drop all rows with NaN. DUe to the large number of data se have. This will hel reduce computation cost
new_df =new_df.dropna()


#all our columns are uniform
new_df.info()

to_use = ['loan_status' , 'loan_amnt' , 'int_rate' , 'emp_length' , 'home_ownership' , 
               'annual_inc' , 'term', 'purpose', 'application_type']

data = new_df[to_use]

data.head()

col = [
  'term',
 'emp_length',
 'home_ownership',
 'purpose',
 'application_type',
 ]

le = LabelEncoder()
for cols in col:
  data[cols] = le.fit_transform(data[cols])

# class_label = class_label.isin(['Default', 'Late (16-30 days)', 'In Grace Period', 'Late (31-120 days)', 'Charged Off']).astype('int')
data['loan_status'] = data['loan_status'].replace({'Default':1, 'Late (16-30 days)':1,'In Grace Period':1,'Late (31-120 days)': 1,'Charged Off': 1, 'Fully Paid':0, 'Current':0})

class_label = data['loan_status']
data.drop(columns=['loan_status'], inplace=True)

trainX, valX, trainy, valy = train_test_split(data, class_label, test_size=0.2, random_state=27)

from imblearn.over_sampling import SMOTE

smote = SMOTE(ratio='minority')
X_sm, y_sm = smote.fit_sample(trainX.values, trainy.values)

model = XGBClassifier()
model.fit(X_sm, y_sm)

# from sklearn.ensemble import RandomForestClassifier

# clf=RandomForestClassifier(class_weight='balanced',
#                            n_estimators=2000,
#                            max_depth=4,
#                            max_leaf_nodes=4,
#                            min_samples_split=3,
#                            oob_score=True,
#                            min_samples_leaf=50,
#                            random_state=1)
# clf.fit(trainX, trainy)

# y_predd = clf.predict(valX)

# acc_tests = accuracy_score(valy, y_predd)
# prec_tests = precision_score(valy, y_predd, average='micro')
# rec_tests = recall_score(valy, y_predd, average='micro')


# print(f'''
# Accuracy (test): {acc_tests:.3f}
# Precision (test): {prec_tests:.3f}
# Recall (test): {rec_tests:.3f}
# ''')


y_predd = model.predict(valX.values)

acc_tests = accuracy_score(valy, y_predd)
prec_tests = precision_score(valy, y_predd, average='micro')
rec_tests = recall_score(valy, y_predd, average='micro')


print(f'''
Accuracy (test): {acc_tests:.3f}
Precision (test): {prec_tests:.3f}
Recall (test): {rec_tests:.3f}
''')

from sklearn.metrics import classification_report

print(classification_report(valy, y_predd))

print(f1_score(valy, y_predd, average='micro'))

proba = model.predict_proba(valX.values)[:,1]
proba

cm_test = confusion_matrix(valy, y_predd)
ax = sns.heatmap(cm_test, cmap='viridis_r', annot=True, fmt='d', square=True)
ax.set_xlabel('Predicted')
ax.set_ylabel('True');

feature_importances = pd.DataFrame(model.feature_importances_,
                                   index = valX.columns,
                                    columns=['importance']).sort_values('importance',ascending=False)


feature_importances.head(10)

import pickle

filename = 'credit_model.sav'
pickle.dump(model, open(filename, 'wb'))

loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.predict_proba(valX)[:,1]
result